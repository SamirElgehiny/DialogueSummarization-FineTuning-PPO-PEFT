# DialogueSummarization-FineTuning-PPO-PEFT

Welcome to the repository containing code for three labs focusing on generative AI for dialogue summarization. Each lab explores different aspects, from zero-shot inference to fine-tuning models with reinforcement learning and prompt engineering.

## Overview

1. **Generative AI Use Case: Summarize Dialogue**
   - Explore the impact of input text on model output.
   - Compare zero-shot, one-shot, and few-shot inferences.
   - Take the first step towards prompt engineering.

2. **Fine-Tune a Generative AI Model for Dialogue Summarization**
   - Utilize the FLAN-T5 model for enhanced dialogue summarization.
   - Perform full fine-tuning and evaluate with ROUGE metrics.
   - Experiment with PEFT fine-tuning and assess its benefits.

3. **Fine-Tune FLAN-T5 with Reinforcement Learning (PPO) and PEFT**
   - Detoxify summaries using a Hate Speech Reward Model.
   - Apply Proximal Policy Optimization (PPO) for fine-tuning.
   - Combine PPO with PEFT to generate less-toxic summaries.

## Code Structure
  
- `Lab1-ZeroShot_OneShot_FewShot.ipynb`: Code for the first lab.
- `Lab2-FineTune_DialogueSummarization.ipynb`: Code for the second lab.
- `Lab3-ReinforcementLearning_PPO_PEFT.ipynb`: Code for the third lab.
